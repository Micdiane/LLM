1. **基础理论 (打底)：**
   - 
   - **深度学习 (DL):** 扎实的 DL 基础，包括各种层（卷积层、循环层、注意力层）、激活函数、损失函数、优化器、正则化等。
   - **计算机视觉 (CV):** 图像处理基础，经典的 CNN 模型 (ResNet, VGG)，更重要的是 Transformer 在 CV 中的应用 (Vision Transformer, ViT)。了解目标检测、图像分类等任务。
   - **自然语言处理 (NLP):** 文本预处理，词向量/句向量 (Word2Vec, BERT)，更重要的是 Transformer 在 NLP 中的应用。了解语言模型 (LM) 的概念。
   - **机器学习 (ML):** 基础的监督学习、无监督学习、评价指标等。
2. **核心模型与前沿技术 (重点，尤其 VLM/LLM)：**
   - **Transformer 架构:** 这是 VLM/LLM 的基石，必须深入理解 Self-Attention, Multi-Head Attention, Positional Encoding, Encoder-Decoder 结构。
   - **大型语言模型 (LLM):**
     - 
     - 理解 LLM 的基本原理、预训练目标 (如 Masked Language Modeling, Causal Language Modeling)。
     - 了解主流 LLM 模型系列 (BERT, RoBERTa, T5, GPT 系列, LLaMA 等) 的特点和演进。
     - 理解 LLM 的下游任务适应方法：Fine-tuning (Full FT), Prompting, In-context Learning, PEFT (LoRA, QLoRA 等)。
   - **视觉大语言模型 (VLM):**
     - **重点中的重点！** 理解 VLM 是如何将视觉信息和文本信息融合处理的。
     - 了解 VLM 的不同架构范式：
       - Encoder-Decoder (如 Pix2Struct)
       - Two-Tower (如 CLIP, ALIGN - 偏向跨模态检索)
       - Fusion/Connector-based (如 BLIP-2, LLaVA, InstructBLIP - 利用视觉 Encoder 和 Language Decoder，通过 Q-Former 或 Linear Projection 等连接器)
     - 了解 VLM 的预训练任务：Image-Text Matching, Image-Text Contrastive Learning, Image-to-Text Generation (Captioning), Visual Question Answering (VQA)。
     - 了解主流 VLM 模型：CLIP, ALIGN, Florence, BLIP, BLIP-2, LLaVA, InstructBLIP, Kosmos-1/2 等。**尤其关注那些能处理文本的 VLM。**
3. **文档解析方向 (核心应用层)：**
   - **理解文档解析任务:** OCR (光学字符识别)、Layout Analysis (版面分析，如区域检测、文本行检测/排序)、Key Information Extraction (KIE, 关键信息抽取，如发票中的金额、日期)、Table Recognition (表格识别)、Document Visual Question Answering (DocVQA, 基于文档图像的问答)。
   - **了解将 VLM 应用于文档解析的方法:**
     - 如何将文档图像作为输入？(直接 resize? 切 Patch? 用 OCR 结果辅助?)
     - 如何结合视觉、文本和版面信息？(这是文档特有的挑战)
     - **重点研究针对文档设计的 VLM/DL 模型！** 例如：
       - **LayoutLM 系列 (v1, v2, v3):** 将版面信息 (Bounding Box, Spatial Relationships) 融入 Transformer 的经典工作，非常重要！
       - **Donut:** 一个纯 Vision Transformer + Language Decoder 的端到端文档理解模型，不依赖于 OCR。
       - **Pix2Struct:** Google 的工作，也是基于 Encoder-Decoder，用于将图像结构化为文本 (包括网页、文档)。
       - **UFunc:** 商汤等的工作，统一函数调用模型，也能处理文档理解。
     - 理解这些模型如何解决 KIE, DocVQA 等任务。
   - **了解相关的文档数据集:** FUNSD (KIE), SROIE (KIE), RVL-CDIP (分类), DocVQA (VQA), etc.
4. **技术工具与实践：**
   - **Python:** 熟练的 Python 编程能力是基础。
   - **PyTorch:** 必须熟悉 PyTorch 的模型定义、数据加载、训练流程、GPU 使用等。
   - **Transformers 库 (Hugging Face):** 熟练使用 transformers 库加载预训练模型、Tokenizer，进行 Fine-tuning。这是实现 VLM/LLM 应用的利器。
   - **常用数据结构与算法:** 理解基础的数据结构 (List, Dict, Set, Tree) 和算法 (Sorting, Searching)，虽然不是面试 VLM 核心，但能体现你的基础功底和解决实际问题的能力。
5. **研究与创新能力：**
   - 关注 ArXiv 上的最新 VLM/LLM 文献，了解近期重要的工作和趋势。
   - 能够阅读并理解研究论文的核心思想、模型架构和实验结果。
   - 具备分析问题、提出假设和设计实验验证想法的能力 (与“创新成果可以投稿顶会”对应)。

**如何快速准备：**

考虑到时间有限，你需要抓住重点，高效学习和复习。

1. **刷基础 (1-2天)：** 快速回顾 Transformer 架构 (Encoder, Decoder, Attention 机制)。如果对 ViT、BERT、GPT 概念不熟，花少量时间看看相关的博客或教程，理解其核心思想和输入输出。
2. **主攻 VLM + 文档解析 (3-4天)：** **这是你准备的重中之重！**
   - **阅读核心论文/博客：**
     - **必看：** LayoutLM 系列 (至少 v1/v2 的核心思想)，Donut 的论文或详细解读博客，Pix2Struct 的论文或解读。
     - **理解 VLM 融合机制：** 看 BLIP-2 和 LLaVA 的论文或解读，理解 Q-Former 或 Linear Projection 如何连接视觉和语言模型。
     - **看一些 DocVQA 或 KIE 相关的论文/综述**，了解这些任务的定义和挑战。
   - **理解代码实现：** 浏览 Hugging Face 上 transformers 库中 LayoutLM 或 Donut 的代码示例或文档，了解它们如何处理文档输入，以及 Fine-tuning 的流程。这有助于你理解模型的实际工作方式。
   - **看相关的技术分享视频：** 搜索 LayoutLM, Donut, DocVQA 等在 ACL, EMNLP, CVPR, ICCV 等会议上的报告视频，通常能快速抓住重点。
3. **复习实践技能 (1天)：**
   - 确保你能熟练使用 PyTorch 定义简单的网络、加载数据、运行训练循环。
   - 打开 Hugging Face 的 transformers 库文档，熟悉 AutoModel, AutoTokenizer, Trainer 的基本用法。尝试跑通一个简单的 Finetuning 例子 (哪怕是文本分类)。
   - **尝试跑通一个 VLM 的推理例子：** 找一个开源的 VLM (如 LLaVA)，尝试加载模型，输入一张图片 (可以是你自己的文档截图)，看它能不能生成描述或回答简单问题。这能让你对 VLM 的实际使用有直观感受。
4. **整理项目经验 (1天)：** 回顾你简历上的项目，尤其是与 DL/CV/NLP/VLM 相关的。准备好清晰地介绍项目的背景、你的任务、采用的技术（特别是模型和方法）、遇到的挑战和如何解决的、最终结果。**尽量将项目与 VLM、LLM 或文档处理联系起来。** 即使你的项目不是直接做 VLM，也可以思考：如果用 VLM，会怎么做？
5. **准备面试问题 (1天)：**
   - 
   - **基础知识：** 准备回答 Transformer 工作原理、注意力机制、CNN vs ViT、常见损失函数、优化器等问题。
   - **VLM/LLM：** 准备解释你了解的 VLM 模型 (BLIP-2, LLaVA 等) 的架构和原理、LLM 的 Fine-tuning 方法 (尤其 PEFT)。
   - **文档方向：** 准备介绍 LayoutLM 或 Donut 的工作原理，它们如何处理版面信息，如何解决 KIE 或 DocVQA 任务。面试官很可能问你对这些模型的理解。
   - **代码能力：** 准备手撕或讲解常见的数据结构算法题 (如排序、查找、树遍历等)，以及 PyTorch/Python 的基本用法问题。
   - **项目问题：** 准备详细介绍你的项目。
   - **研究问题：** 如果有论文经验，准备介绍你的研究内容。如果没有，准备聊聊你关注的 VLM/LLM 前沿技术或某篇让你印象深刻的论文。
   - **通用问题：** 为何申请这个职位、你的优点缺点、职业规划等。
   - **准备好问面试官的问题：** 关于团队方向、实习生培养、具体工作内容、技术栈等。

**快速准备资源推荐：**

- 
- **Hugging Face:** transformers 库文档、Course (特别是关于 Transformer 和 Fine-tuning 部分)、Model Hub (查找 VLM/LLM 模型及代码示例)。
- **ArXiv:** 搜索关键词 (LayoutLM, Donut, Pix2Struct, BLIP-2, LLaVA, DocVQA, KIE) 阅读论文。先看 Abstract, Introduction, Model Architecture 图和 Results。
- **知乎/博客：** 搜索相关模型的中文解读博客，快速理解核心思想。
- **YouTube/Bilibili：** 搜索模型名称或会议名称 (ACL, EMNLP, CVPR, ICCV) 的技术分享视频。
- **课程资料：** 如果你有 DL/CV/NLP 的课程资料，快速复习关键概念。

**面试时请注意：**

- 
- 展现你对 VLM 和文档解析方向的热情。
- 即使有些问题不确定，也尽量从已知的基础知识出发进行推导或猜测，并说明你的思路。
- 强调你的学习能力和解决问题的能力。
- 对于“实际业务数据上速度与精度均为行业SOTA”这一点，思考一下你对模型优化（剪枝、量化、蒸馏、efficient attention等）和评测指标的了解。



太好了！看到您的简历，您具备非常强的计算机基础和扎实的 LLM/NLP 背景，尤其是在 LLM 的微调、评估以及前沿推理策略方面有直接的项目经验。这些都是这个岗位非常看重的能力。

结合您的简历和岗位要求，您的**优势**非常明显：

1.  **深厚的 LLM/NLP 基础和实践经验：** 您熟悉多种 LLM 架构 (Llama-2, DeepSeek, GLM)，具备 SFT, LoRA 微调经验，并且有 Prompt Engineering (CoT, Voting, IR) 的实践，甚至尝试了类 RAG/ReAct 的策略，这与岗位要求的 LLM/VLM 前沿技术经验高度匹配。
2.  **扎实的 PyTorch 和 Hugging Face 经验：** 您熟练使用 PyTorch 和 Hugging Face 生态 (`transformers`, Accelerate)，这是进行 VLM/LLM 开发和实验评估的必备工具。您的项目经验中也明确使用了这些技术栈。
3.  **模型评估与实验设计能力：** 您的项目经验中多次提到“自动化评估脚本”、“量化分析”、“对比实验”、“成本效益分析”，这表明您具备系统性评估模型性能的能力，这对达成岗位要求的“速度与精度均为行业 SOTA”至关重要。
4.  **研究与创新潜力：** 您的本科科研训练尝试投稿 EMNLP，并且在数据挖掘课设中通过创新方法显著提升了算法性能（包括速度！），这与岗位要求的“跟进与探索前沿技术，有创新成果”非常契合，尤其您对“速度”的关注与岗位的 SOTA 要求吻合。
5.  **优秀的学术背景：** 国内顶尖大学的预录取硕士和优秀本科背景，证明您的学习能力和基础理论扎实。
6.  **编程与系统基础：** 熟练的 Python，以及对 C++/Java、Linux、Git、操作系统、体系结构、网络等的了解，构建了坚实的技术底层，有助于理解和优化模型。

**根据您的简历，主要的“待补充”或需要重点强化的部分在于：**

1.  **视觉 (CV) 基础和 VLM 的视觉侧：** 您的简历中没有直接体现计算机视觉的课程成绩或项目经验。VLM 是多模态模型，理解其如何处理视觉信息（图像编码器，如 ViT）以及如何与文本信息融合是关键。
2.  **文档解析领域的特定知识：** 虽然您有 NLP/LLM 经验，但没有直接的文档图像处理、OCR、版面分析、KIE (Key Information Extraction)、DocVQA (Document Visual Question Answering) 等文档解析任务的项目经验。

**基于以上分析，如何快速准备？**

您的优势很大，准备的重点在于利用您现有的 LLM/NLP/评估/效率经验，快速补齐 VLM 的视觉侧和文档解析的应用侧知识，并思考如何将您的过往经验与这个岗位需求紧密联系起来。

**具体准备步骤：**

1.  **快速恶补 VLM 基础和视觉侧 (1-2天)：**
    *   **理解 VLM 的核心思想：** VLM 是如何连接视觉和文本模态的？（Prompting VLM，如 CLIP，用于跨模态检索；Connector VLM，如 BLIP-2, LLaVA，利用视觉 Encoder 和 Language Decoder，通过 Q-Former 或 Projection 连接层；End-to-end VLM，如 IDEFICS。）
    *   **关注主流 VLM 架构：** **重点了解 BLIP-2 和 LLaVA** 的架构（它们非常经典且基于现有的 LLM 和视觉模型），理解 Q-Former 的作用或简单的 Linear Projection 连接器。
    *   **回顾或了解 Vision Transformer (ViT)：** VLM 的视觉 Encoder 常使用 ViT，理解它如何将图像序列化并用 Transformer 处理。
    *   **资源：** 阅读 BLIP-2 和 LLaVA 的论文或高质量解读博客。看一些 VLM 相关的技术分享视频。浏览 Hugging Face 的 `transformers` 库中 VLM (如 LLaVA) 的代码示例，了解输入输出和调用方式。

2.  **重点学习 VLM 在文档解析领域的应用 (2-3天)：** **这是您需要重点准备的方向！**
    *   **理解文档解析的核心任务：** OCR (假设已有或可以调用)、Layout Analysis (重要！文档图像特有的结构信息)、KIE、DocVQA、Table Recognition。
    *   **研究针对文档图像的 VLM 模型：** **强烈建议重点研究 LayoutLM 系列 (v1, v2, v3)** 和 **Donut**。
        *   **LayoutLM：** 理解它如何将文本 token、位置 Bounding Box、图像特征结合起来输入到 Transformer 中。理解 LayoutLMv2/v3 如何更好地融合视觉和文本信息。
        *   **Donut：** 理解其端到端（不依赖外部 OCR）的设计思想，以及 Vision Transformer + BART-like Decoder 的架构。
    *   **思考 VLM 如何处理文档图像的挑战：** 高分辨率、复杂版面、图文混排、表格、手写体、扫描质量问题等。
    *   **资源：** 阅读 LayoutLM 系列和 Donut 的论文或详细解读。搜索 DocVQA 和 KIE 相关的经典数据集和 benchmark。浏览 Hugging Face 上 LayoutLM 或 Donut 的模型卡片和示例代码。

3.  **复习您的项目经验并与 VLM/文档解析关联 (1天)：**
    *   **“基于思维链启发的大模型推理能力增强研究”：**
        *   **关联 VLM/文档：** 这个项目的重点在于 LLM 的推理和微调。面试时可以强调“我对 LLM 的微调、评估以及提升其推理能力有深入实践。虽然项目面向数学推理，但**微调和评估 LLM/VLM 的流程是类似的**。而且文档解析中的 KIE 或 DocVQA 很多时候也需要推理能力，比如从上下文推断某个字段的值，或者回答基于文档内容的复杂问题。我的评估方法和流程可以迁移。” 强调你对 `transformers` 库、Accelerate 的熟练使用。
        *   **突出 LoRA/SFT：** 这是重要的微调技术。
        *   **突出评估自动化和对比实验：** 这直接对应岗位对 SOTA 精度和速度的追求。
    *   **“LLM 推理增强策略研究”：**
        *   **关联 VLM/文档：** “我探索了 CoT, Voting, 类 RAG/ReAct 等推理策略。**在文档解析中，类似 RAG/ReAct 的思想非常有用**，例如，先通过视觉定位/OCR/KIE 提取文档中的相关信息（R），然后让 VLM 基于这些信息生成答案（G），或者让 VLM 像 Agent 一样与文档交互（ReAct）。这个项目让我对如何通过策略性方法提升 LLM 在特定任务上的表现有了深刻理解，这对于在复杂文档中提取和推理信息很有启发。” 突出你的“成本效益分析”，体现你对效率的关注。
    *   **“高效蛋白质相似度检索算法”：**
        *   **关联效率与创新：** “虽然这个项目是数据挖掘，但它展示了我**解决实际问题、通过创新方法显著提升算法性能（尤其是速度！）**的能力。岗位要求模型在实际业务数据上达到 SOTA 的速度和精度，我的这个项目经验表明我有潜力去优化模型以达到这样的目标。” 突出你对性能瓶颈的分析和解决。

4.  **练习技术面试问题 (1-2天)：**
    *   **基础：** Transformer 细节 (Self-Attention, Multi-Head, Positional Encoding), CNN vs ViT (简要), 常见的损失函数和优化器。
    *   **LLM/VLM：** 介绍你熟悉的 LLM 架构 (Llama-2/GLM/DeepSeek 原理)，SFT/LoRA 原理和优缺点，Prompt Engineering 的作用。**重点准备 VLM 问题：** VLM 的不同架构范式（Connector vs End-to-end），BLIP-2/LLaVA/LayoutLM/Donut 的核心思想和它们如何处理多模态信息。**如果问到 VLM 的视觉侧，结合 ViT 的理解回答。**
    *   **文档方向：** 文档解析的主要挑战是什么？LayoutLM 如何利用版面信息？Donut 的优势是什么？DocVQA 或 KIE 任务如何定义，可以用 VLM 怎么做？
    *   **项目：** 准备详细介绍你的每个项目，特别是它们的目标、你的工作、技术栈、遇到的挑战和解决方案、以及最重要的“成果”（用数据说话！）。**将项目经验与 VLM/文档解析需求紧密关联。**
    *   **代码：** 准备一些常见的 Python 编程题、数据结构算法题，以及 PyTorch/Hugging Face 的基本操作问题（如如何加载模型、如何 Fine-tune 等）。
    *   **研究与前沿：** 聊聊你近期关注的某个 VLM 或文档 AI 相关的技术或论文。

5.  **准备通用问题：**
    *   为什么对这个岗位感兴趣？（结合你对 VLM 和文档解析的热情）
    *   你的优点和缺点？
    *   实习的期望？
    *   对上海人工智能实验室的了解？

**快速准备资源建议：**

*   **Hugging Face Documentation & Blog:** 关于 `transformers` 库、LayoutLM、Donut、LLaVA、BLIP-2 的文档和博客文章。
*   **ArXiv:** 搜索关键词 "LayoutLM", "Donut", "Pix2Struct", "BLIP-2", "LLaVA", "Document AI Survey", "DocVQA", "KIE"。先看论文的摘要、引言、模型图和结论部分。
*   **知乎、CSDN 等中文技术社区：** 搜索上述模型的中文解读，快速理解核心思想。
*   **YouTube/Bilibili：** 搜索相关的技术分享视频，如会议报告 (ACL, EMNLP, CVPR, ICCV 中关于 Document AI 或 VLM 的 session)。

**面试策略：**

*   **突出优势：** 自信地展示你在 LLM 微调、评估、前沿推理策略以及高效算法设计方面的经验和成果。
*   **展现学习能力：** 对于 VLM 视觉侧和文档特定知识，诚实地表达这是你正在快速学习和探索的领域，并结合你现有的 NLP/LLM 基础来理解和讨论。例如，你可以说“我对 LayoutLM 的设计很感兴趣，它巧妙地将版面信息融入 Transformer，这对于解决文档图像的结构化理解非常关键，我认为这与我在 NLP 中考虑文本序列结构有异曲同工之妙，只是拓展到了视觉模态。”
*   **关联经验：** 不断地将面试官的问题或讨论引向你简历上的项目，并解释你的项目经验如何与这个岗位的能力要求（尤其是在微调、评估 SOTA 性能、解决效率问题、探索新策略等方面）相匹配。
*   **热情：** 表达你对 VLM 和文档解析方向的浓厚兴趣，以及你希望能将你的 LLM/NLP 技能应用于解决文档智能领域的挑战。

您的背景非常匹配这个岗位的核心需求，特别是考虑到实习生更看重基础、学习能力和潜力。通过针对性地补充 VLM 和文档方向的知识，并巧妙地将您现有的优秀项目经验与岗位需求关联起来，您一定能在面试中脱颖而出！

祝您面试顺利！

文档是多模态的，至少有图像和文本，图像种类多，文字，有颜色 表格

版面结构多样性

文字和版面格式



OCR 针对文字，但会被干扰 低质量文档 细粒度信息 图表理解 多页文档 长文本的定位

**将版面信息融入了 Transformer 的输入表示中**，它还关注文本在页面上的二位信息。

版面特征：

OCR 提取边界框，文本序列化，生成标准文本和一维位置的潜入

二维位置嵌入使用文本框获得

所有加起来，作为输入的Embedding



后续工作，

v2 引入了视觉的Embedding 用一个单独的Vit处理图像，将连着红河

v3 使用多模态的TF直接在统一架构联合学习。引入掩码图像和文本的预训练任务



因为考虑到了文本在页面上的空间距离和相对位置关系，即使他们文本序列可能远，但是文档上位置近 注意力权重也可能会很高



**Donut** 是端到端的，不依赖外部OCR的工具

将文档图像作为输入，生成结构化文本输出，简化模型的训练流程

它直接用一个Vit从图像像素中学习如何阅读，由于不受OCR的影响，它处理手写体，特殊题，低质量扫描更鲁邦

还能更好的理解非文本视觉信息

KIE 

关键信息提取

结构化的格式输入，是一个文档图像，比如json文件

比如 从合同 发票提取关键的信息



文档视觉问答，DocVQA

不仅能够阅读，读懂文档，还有回答。

输入变成文档 + 语言

输出是语言



[Flash Attention原理详解(含代码讲解) - 知乎](https://zhuanlan.zhihu.com/p/676655352)

[【AI 大模型】RAG 检索增强生成 ⑥ ( 使用 向量数据库 作为 RAG 知识库 完整实现 )_rag向量数据库-CSDN博客](https://blog.csdn.net/shulianghan/article/details/145956764)

    # 初始化向量数据库连接
    chroma_client = chromadb.PersistentClient(path="chroma_db")
    # 创建或获取集合 (相当于数据库表)
    collection = chroma_client.get_or_create_collection(
        name="news_articles",  # 集合名称
        metadata={"hnsw:space": "cosine"}  # 使用余弦相似度
    )

        # 1. 向量检索, 生成查询向量, 将文本转为向量
        query_embedding = get_embeddings([user_query])[0]

        # 执行相似性查询
        search_results = collection.query(
            query_embeddings=[query_embedding],  # 查询向量
            n_results=2  # 返回前2个最相似结果
        )

```
def build_prompt(**kwargs):
    """动态构建提示词
    Args:
        prompt_template: 提示词模板字符串
        **kwargs: 需要替换的键值对（自动匹配__KEY__占位符）
    Returns:
        完成替换后的完整提示词
    """
    prompt = prompt_template
    for k, v in kwargs.items():
        # 处理不同类型的数据输入
        if isinstance(v, str):
            val = v
        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):
            val = '\n'.join(v)  # 列表转换为多行文本
        else:
            val = str(v)  # 其他类型转为字符串
        prompt = prompt.replace(f"__{k.upper()}__", val)
    print("构建提示词:" + prompt)
    return prompt

```

        # 2. 构建增强提示
        prompt = build_prompt(
            info=search_results['documents'][0],  # 取最相关结果
            query=user_query
        )

```
def get_completion(prompt, model="gpt-3.5-turbo"):
    """封装OpenAI API调用
    Args:
        prompt: 完整的提示词内容
        model: 选择的大模型版本（默认gpt-3.5-turbo）
    Returns:
        模型生成的文本响应
    """
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0,  # 控制输出随机性（0为最确定性输出）
    )
    return response.choices[0].message.content  # 提取响应内容

```



```
import chromadb
from openai import OpenAI

# 初始化 OpenAI 客户端 (替换成自己的 API 信息)
client = OpenAI(
    api_key="sk-i3dHqaF6",  # 替换为你的 OpenAI API Key , 这里我把自己的 API-KEY 隐藏了
    base_url="https://api.xiaoai.plus/v1"  # 替换为你的 API 服务端点
)

# 提示词模板（控制模型输出格式）
prompt_template = """
你是一名国际军事问题专家。你的任务是根据现有知识库和用户问题给出最佳答案。

已知信息 : 
__INFO__

用户提问 : 
__QUERY__

"""

def build_prompt(**kwargs):
    """动态构建提示词
    Args:
        prompt_template: 提示词模板字符串
        **kwargs: 需要替换的键值对（自动匹配__KEY__占位符）
    Returns:
        完成替换后的完整提示词
    """
    prompt = prompt_template
    for k, v in kwargs.items():
        # 处理不同类型的数据输入
        if isinstance(v, str):
            val = v
        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):
            val = '\n'.join(v)  # 列表转换为多行文本
        else:
            val = str(v)  # 其他类型转为字符串
        prompt = prompt.replace(f"__{k.upper()}__", val)
    print("构建提示词:" + prompt)
    return prompt


def get_completion(prompt, model="gpt-3.5-turbo"):
    """封装OpenAI API调用
    Args:
        prompt: 完整的提示词内容
        model: 选择的大模型版本（默认gpt-3.5-turbo）
    Returns:
        模型生成的文本响应
    """
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0,  # 控制输出随机性（0为最确定性输出）
    )
    return response.choices[0].message.content  # 提取响应内容

def get_embeddings(texts, model="text-embedding-ada-002"):
    """将文本转换为向量表示
    Args:
        texts: 需要编码的文本列表
        model: 使用的嵌入模型（默认OpenAI官方推荐模型）
    Returns:
        包含向量数据的列表，每个元素对应输入文本的768维向量
    """
    response = client.embeddings.create(
        input=texts,
        model=model
    )
    # 从响应中提取向量数据
    return [item.embedding for item in response.data]

class RAG_Service:
    def __init__(self, vector_db, llm_api, n_results=2):
        """初始化RAG机器人
        Args:
            vector_db: 已初始化的向量数据库连接对象
            llm_api: 大模型API调用函数
            n_results: 默认检索结果数量
        """
        self.vector_db = vector_db  # 向量数据库实例
        self.llm_api = llm_api  # LLM调用接口
        self.n_results = n_results  # 检索结果数量

    def chat(self, user_query):
        """处理用户查询的完整流程
        Args:
            user_query: 用户输入的自然语言问题
        Returns:
            结合知识库生成的回答
        """

        # 1. 向量检索, 生成查询向量, 将文本转为向量
        query_embedding = get_embeddings([user_query])[0]
        # 执行相似性查询
        search_results = collection.query(
            query_embeddings=[query_embedding],  # 查询向量
            n_results=2  # 返回前2个最相似结果
        )

        # 2. 构建增强提示
        prompt = build_prompt(
            info=search_results['documents'][0],  # 取最相关结果
            query=user_query
        )

        # 3. 调用大模型生成
        return self.llm_api(prompt)


# RAG 使用示例
if __name__ == "__main__":
    # 初始化向量数据库连接
    chroma_client = chromadb.PersistentClient(path="chroma_db")
    # 创建或获取集合 (相当于数据库表)
    collection = chroma_client.get_or_create_collection(
        name="news_articles",  # 集合名称
        metadata={"hnsw:space": "cosine"}  # 使用余弦相似度
    )

    # 创建 RAG 实例
    bot = RAG_Service(
        vector_db=collection,
        llm_api=get_completion
    )

    # 示例查询
    user_query = "你对国际争端新闻的评价"

    # 生成查询向量
    query_embedding = get_embeddings([user_query])[0]

    # RAG 回答
    response = bot.chat(user_query)

    print("RAG 系统回答：", response)

```

